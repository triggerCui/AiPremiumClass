from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset, DataLoader
import torch

# 1. 加载分词器和模型
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
# 冻结BERT参数（示例）
model_frozen = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=len(label_classes))
for param in model_frozen.bert.parameters():
    param.requires_grad = False  # 冻结BERT层

# 不冻结模型（直接加载）
model_untrained = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=len(label_classes))

# 2. 数据预处理函数
def tokenize_function(examples):
    return tokenizer(examples['comment'], padding='max_length', truncation=True, max_length=128)

# 3. 定义训练参数（含TensorBoard跟踪）
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',  # TensorBoard日志路径
    logging_steps=100,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    report_to='tensorboard',  # 启用TensorBoard
)

# 4. 初始化Trainer并训练
trainer_frozen = Trainer(
    model=model_frozen,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics  # 自定义评估指标（如准确率、F1值）
)
trainer_frozen.train()

# 对不冻结模型重复上述流程
trainer_untrained = Trainer(
    model=model_untrained,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)
trainer_untrained.train()
